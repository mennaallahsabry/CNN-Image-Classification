{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 | Import Necessary Libraries**  (TensorFlow and Keras to handle the image data)"
      ],
      "metadata": {
        "id": "n-3VJNkXjZZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j2LL19o-UZeX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.models import load_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 | Data Preparation and Exploration**"
      ],
      "metadata": {
        "id": "HiLGGwrCjuat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the CIFAR-10 dataset from Keras library:"
      ],
      "metadata": {
        "id": "O4htRZrAj3In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd_HFpuaUdby",
        "outputId": "2bfdb5e9-c682-45a0-ea01-893d05f0c59f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split original training data to training and validation sets"
      ],
      "metadata": {
        "id": "gOw7L_mlkEYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "SZyADOENUg_o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing out the dimensions of our training, validation, and test datasets"
      ],
      "metadata": {
        "id": "gB86-LdPkNWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Images Shape:      ', X_train.shape)\n",
        "print('Train Labels Shape:      ', y_train.shape)\n",
        "\n",
        "print('\\nValidation Images Shape: ', X_valid.shape)\n",
        "print('Validation Labels Shape: ', y_valid.shape)\n",
        "\n",
        "print('\\nTest Images Shape:       ', X_test.shape)\n",
        "print('Test Labels Shape:       ', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umZc5KmRUjsN",
        "outputId": "a3e1441d-ed2a-4c33-914f-a1ca8449dbe9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Images Shape:       (45000, 32, 32, 3)\n",
            "Train Labels Shape:       (45000, 1)\n",
            "\n",
            "Validation Images Shape:  (5000, 32, 32, 3)\n",
            "Validation Labels Shape:  (5000, 1)\n",
            "\n",
            "Test Images Shape:        (10000, 32, 32, 3)\n",
            "Test Labels Shape:        (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An overview of the CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "tmYcZ77bkQQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 classes\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "# Loop over the first 25 images\n",
        "for i in range(64):\n",
        "    # Create a subplot for each image\n",
        "    plt.subplot(8, 8, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "# CIFAR-10 classes\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "# Loop over the first 25 images\n",
        "for i in range(64):\n",
        "    # Create a subplot for each image\n",
        "    plt.subplot(8, 8, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DLZwjXhdUoYz",
        "outputId": "6aa1a117-2c1b-4c6a-aaa1-f5282e249712"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 64 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAASXCAYAAACgHLUiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqIElEQVR4nO3cMWojWRhG0SfTqeRcWPtfmEELUOXSRG4mcw3zX8Y9dU4sigJ9PB4X2afX6/VaAAAAADDs7b9+AQAAAAD+n4QnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASPza86Hn87nu9/s6n8/rdDrV78R/7PV6rW3b1vV6XW9v823Sno6l3tNaNnUk9sQ0m2KSPTHNvZxJziim7d3UrvB0v9/X7XYbezn+DJ+fn+vj42P8ufZ0TNWe1rKpI7InptkUk+yJae7lTHJGMe27Te0KT+fz+ffDLpfLzJvxYz0ej3W73X5/79Ps6VjqPa1lU0diT0yzKSbZE9Pcy5nkjGLa3k3tCk9fP5G7XC7GcyDVTyPt6ZjKn9ra1PHYE9Nsikn2xDT3ciY5o5j23ab8c3EAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEj82vOh1+u11lrr8XikL8PP8PU9f33v0+zpWOo9/f3ZNvX/Z09Msykm2RPT3MuZ5Ixi2t5N7QpP27attda63W7/8rX4k2zbtt7f35PnrmVPR1Pt6evZa9nUkdgT02yKSfbENPdyJjmjmPbdpk6vHbnz+Xyu+/2+zufzOp1Ooy/Iz/N6vda2bet6va63t/m/xrSnY6n3tJZNHYk9Mc2mmGRPTHMvZ5Iziml7N7UrPAEAAADAP+WfiwMAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABK/9nzo+Xyu+/2+zufzOp1O9TvxH3u9XmvbtnW9Xtfb23ybtKdjqfe0lk0diT0xzaaYZE9Mcy9nkjOKaXs3tSs83e/3dbvdxl6OP8Pn5+f6+PgYf649HVO1p7Vs6ojsiWk2xSR7Ypp7OZOcUUz7blO7wtP5fP79sMvlMvNm/FiPx2Pdbrff3/s0ezqWek9r2dSR2BPTbIpJ9sQ093ImOaOYtndTu8LT10/kLpeL8RxI9dNIezqm8qe2NnU89sQ0m2KSPTHNvZxJziimfbcp/1wcAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASv/Z86PV6rbXWejwe6cvwM3x9z1/f+zR7OpZ6T39/tk39/9kT02yKSfbENPdyJjmjmLZ3U7vC07Zta621brfbv3wt/iTbtq339/fkuWvZ09FUe/p69lo2dST2xDSbYpI9Mc29nEnOKKZ9t6nTa0fufD6f636/r/P5vE6n0+gL8vO8Xq+1bdu6Xq/r7W3+rzHt6VjqPa1lU0diT0yzKSbZE9Pcy5nkjGLa3k3tCk8AAAAA8E/55+IAAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABA4teeDz2fz3W/39f5fF6n06l+J/5jr9drbdu2rtfrenubb5P2dCz1ntayqSOxJ6bZFJPsiWnu5UxyRjFt76Z2haf7/b5ut9vYy/Fn+Pz8XB8fH+PPtadjqva0lk0dkT0xzaaYZE9Mcy9nkjOKad9tald4Op/Pvx92uVxm3owf6/F4rNvt9vt7n2ZPx1LvaS2bOhJ7YppNMcmemOZeziRnFNP2bmpXePr6idzlcjGeA6l+GmlPx1T+1NamjseemGZTTLInprmXM8kZxbTvNuWfiwMAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQOLXng+9Xq+11lqPxyN9GX6Gr+/563ufZk/HUu/p78+2qf8/e2KaTTHJnpjmXs4kZxTT9m5qV3jatm2ttdbtdvuXr8WfZNu29f7+njx3LXs6mmpPX89ey6aOxJ6YZlNMsiemuZczyRnFtO82dXrtyJ3P53Pd7/d1Pp/X6XQafUF+ntfrtbZtW9frdb29zf81pj0dS72ntWzqSOyJaTbFJHtimns5k5xRTNu7qV3hCQAAAAD+Kf9cHAAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkPi150PP53Pd7/d1Pp/X6XSq34n/2Ov1Wtu2rev1ut7e5tukPR1Lvae1bOpI7IlpNsUke2KaezmTnFFM27upXeHpfr+v2+029nL8GT4/P9fHx8f4c+3pmKo9rWVTR2RPTLMpJtkT09zLmeSMYtp3m9oVns7n8++HXS6XmTfjx3o8Hut2u/3+3qfZ07HUe1rLpo7EnphmU0yyJ6a5lzPJGcW0vZvaFZ6+fiJ3uVyM50Cqn0ba0zGVP7W1qeOxJ6bZFJPsiWnu5UxyRjHtu0355+IAAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJD4tedDr9drrbXW4/FIX4af4et7/vrep9nTsdR7+vuzber/z56YZlNMsiemuZczyRnFtL2b2hWetm1ba611u93+5WvxJ9m2bb2/vyfPXcuejqba09ez17KpI7EnptkUk+yJae7lTHJGMe27TZ1eO3Ln8/lc9/t9nc/ndTqdRl+Qn+f1eq1t29b1el1vb/N/jWlPx1LvaS2bOhJ7YppNMcmemOZeziRnFNP2bmpXeAIAAACAf8o/FwcAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASv/Z86Pl8rvv9vs7n8zqdTvU78R97vV5r27Z1vV7X29t8m7SnY6n3tJZNHYk9Mc2mmGRPTHMvZ5Iziml7N7UrPN3v93W73cZejj/D5+fn+vj4GH+uPR1Ttae1bOqI7IlpNsUke2KaezmTnFFM+25Tu8LT+Xz+/bDL5TLzZvxYj8dj3W6339/7NHs6lnpPa9nUkdgT02yKSfbENPdyJjmjmLZ3U7vC09dP5C6Xi/EcSPXTSHs6pvKntjZ1PPbENJtikj0xzb2cSc4opn23Kf9cHAAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEr/2fOj1eq211no8HunL8DN8fc9f3/s0ezqWek9/f7ZN/f/ZE9Nsikn2xDT3ciY5o5i2d1O7wtO2bWuttW632798Lf4k27at9/f35Llr2dPRVHv6evZaNnUk9sQ0m2KSPTHNvZxJziimfbep02tH7nw+n+t+v6/z+bxOp9PoC/LzvF6vtW3bul6v6+1t/q8x7elY6j2tZVNHYk9Msykm2RPT3MuZ5Ixi2t5N7QpPAAAAAPBP+efiAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAxK89H3o+n+t+v6/z+bxOp1P9TvzHXq/X2rZtXa/X9fY23ybt6VjqPa1lU0diT0yzKSbZE9Pcy5nkjGLa3k3tCk/3+33dbrexl+PP8Pn5uT4+Psafa0/HVO1pLZs6Intimk0xyZ6Y5l7OJGcU077b1K7wdD6ffz/scrnMvBk/1uPxWLfb7ff3Ps2ejqXe01o2dST2xDSbYpI9Mc29nEnOKKbt3dSu8PT1E7nL5WI8B1L9NNKejqn8qa1NHY89Mc2mmGRPTHMvZ5Izimnfbco/FwcAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgMSvPR96vV5rrbUej0f6MvwMX9/z1/c+zZ6Opd7T359tU/9/9sQ0m2KSPTHNvZxJziim7d3UrvC0bdtaa63b7fYvX4s/ybZt6/39PXnuWvZ0NNWevp69lk0diT0xzaaYZE9Mcy9nkjOKad9t6vTakTufz+e63+/rfD6v0+k0+oL8PK/Xa23btq7X63p7m/9rTHs6lnpPa9nUkdgT02yKSfbENPdyJjmjmLZ3U7vCEwAAAAD8U/65OAAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJD4tedDz+dz3e/3dT6f1+l0qt+J/9jr9Vrbtq3r9bre3ubbpD0dS72ntWzqSOyJaTbFJHtimns5k5xRTNu7qV3h6X6/r9vtNvZy/Bk+Pz/Xx8fH+HPt6ZiqPa1lU0dkT0yzKSbZE9Pcy5nkjGLad5vaFZ7O5/Pvh10ul5k348d6PB7rdrv9/t6n2dOx1Htay6aOxJ6YZlNMsiemuZczyRnFtL2b2hWevn4id7lcjOdAqp9G2tMxlT+1tanjsSem2RST7Ilp7uVMckYx7btN+efiAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQ+LXnQ6/Xa6211uPxSF+Gn+Hre/763qfZ07HUe/r7s23q/8+emGZTTLInprmXM8kZxbS9m9oVnrZtW2utdbvd/uVr8SfZtm29v78nz13Lno6m2tPXs9eyqSOxJ6bZFJPsiWnu5UxyRjHtu02dXjty5/P5XPf7fZ3P53U6nUZfkJ/n9XqtbdvW9Xpdb2/zf41pT8dS72ktmzoSe2KaTTHJnpjmXs4kZxTT9m5qV3gCAAAAgH/KPxcHAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkfu350PP5XPf7fZ3P53U6nep34j/2er3Wtm3rer2ut7f5NmlPx1LvaS2bOhJ7YppNMcmemOZeziRnFNP2bmpXeLrf7+t2u429HH+Gz8/P9fHxMf5cezqmak9r2dQR2RPTbIpJ9sQ093ImOaOY9t2mdoWn8/n8+2GXy2XmzfixHo/Hut1uv7/3afZ0LPWe1rKpI7EnptkUk+yJae7lTHJGMW3vpnaFp6+fyF0uF+M5kOqnkfZ0TOVPbW3qeOyJaTbFJHtimns5k5xRTPtuU/65OAAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJIQnAAAAABLCEwAAAAAJ4QkAAACAhPAEAAAAQEJ4AgAAACAhPAEAAACQEJ4AAAAASAhPAAAAACSEJwAAAAASwhMAAAAACeEJAAAAgITwBAAAAEBCeAIAAAAgITwBAAAAkBCeAAAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAICE8AQAAAJAQngAAAABICE8AAAAAJH7t+dDr9VprrfV4PNKX4Wf4+p6/vvdp9nQs9Z7+/myb+v+zJ6bZFJPsiWnu5UxyRjFt76Z2hadt29Zaa91ut3/5WvxJtm1b7+/vyXPXsqejqfb09ey1bOpI7IlpNsUke2KaezmTnFFM+25Tp9eO3Pl8Ptf9fl/n83mdTqfRF+Tneb1ea9u2db1e19vb/F9j2tOx1Htay6aOxJ6YZlNMsiemuZczyRnFtL2b2hWeAAAAAOCf8s/FAQAAAEgITwAAAAAkhCcAAAAAEsITAAAAAAnhCQAAAICE8AQAAABAQngCAAAAIPEXjA3wfYZurCwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 64 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 | Data Preprocessing**"
      ],
      "metadata": {
        "id": "Zeg-bFqGkW9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization of Image Data (convert the pixel values data type to float32 type, and then normalizes them by subtracting the mean and dividing by the standard deviation of the training set)"
      ],
      "metadata": {
        "id": "DaM814oFkfYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pixel values data type to float32\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "X_valid = X_valid.astype('float32')\n",
        "\n",
        "# Calculate the mean and standard deviation of the training images\n",
        "mean = np.mean(X_train)\n",
        "std  = np.std(X_train)\n",
        "\n",
        "# Normalize the data\n",
        "# The tiny value 1e-7 is added to prevent division by zero\n",
        "X_train = (X_train-mean)/(std+1e-7)\n",
        "X_test  = (X_test-mean) /(std+1e-7)\n",
        "X_valid = (X_valid-mean)/(std+1e-7)"
      ],
      "metadata": {
        "id": "BBSTegrDUvHc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding of Labels convert the class labels to one-hot vectors to transform the categorical labels into a format suitable"
      ],
      "metadata": {
        "id": "KhlTHbdCkmol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, 10)\n",
        "y_valid = to_categorical(y_valid, 10)\n",
        "y_test  = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "Ybat-PzkUxjq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation** expand the size of the training set by creating modified versions of images in the dataset.\n",
        "Data augmentation techniques such as rotations, shifts, flips, shearing, and intensity changes introduce small variations to the existing images, creating a broader set of training samples to learn from."
      ],
      "metadata": {
        "id": "wzuheRHpkuzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "    # Rotate images randomly by up to 15 degrees\n",
        "    rotation_range=15,\n",
        "\n",
        "    # Shift images horizontally by up to 12% of their width\n",
        "    width_shift_range=0.12,\n",
        "\n",
        "    # Shift images vertically by up to 12% of their height\n",
        "    height_shift_range=0.12,\n",
        "\n",
        "    # Randomly flip images horizontally\n",
        "    horizontal_flip=True,\n",
        "\n",
        "    # Zoom images in by up to 10%\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    # Change brightness by up to 10%\n",
        "    brightness_range=[0.9,1.1],\n",
        "\n",
        "    # Shear intensity (shear angle in counter-clockwise direction in degrees)\n",
        "    shear_range=10,\n",
        "\n",
        "    # Channel shift intensity\n",
        "    channel_shift_range=0.1,\n",
        ")"
      ],
      "metadata": {
        "id": "VSrZV0omU2bm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4 | Define CNN Model Architecture**"
      ],
      "metadata": {
        "id": "AwALufV1lWjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A brief explanation of the architecture:**\n",
        "\n",
        "**1-**The network begins with a pair of Conv2D layers, each with 32 filters of size 3x3. This is followed by a Batch Normalization layer which accelerates training and provides some level of regularization, helping to prevent overfitting.\n",
        "\n",
        "**2-** The pairs of Conv2D layers are followed by a MaxPooling2D layer, which reduces the spatial dimensions (height and width), effectively providing a form of translation invariance and reducing computational complexity. This is followed by a Dropout layer that randomly sets a fraction (0.2 for the first dropout layer) of the input units to 0 at each update during training, helping to prevent overfitting.\n",
        "\n",
        "**3-** This pattern of two Conv2D layers, followed by a Batch Normalization layer, a MaxPooling2D layer, and a Dropout layer, repeats three more times. The number of filters in the Conv2D layers doubles with each repetition, starting from 32 and going up to 64, 128, and then 256. This increasing pattern helps the network to learn more complex features at each level. The dropout rate also increases at each step, from 0.2 to 0.5.\n",
        "\n",
        "**4-** After the convolutional and pooling layers, a Flatten layer is used to convert the 2D outputs of the preceding layer into a 1D vector.\n",
        "\n",
        "**5-** Finally, a Dense (or fully connected) layer is used for classification. It has 10 units, each representing one of the 10 classes of the CIFAR-10 dataset, and a softmax activation function is used to convert the outputs to probability scores for each class\n",
        "\n"
      ],
      "metadata": {
        "id": "F1B4QLpelcp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Set the weight decay value for L2 regularization\n",
        "weight_decay = 0.0001\n",
        "\n",
        "# Add the first convolutional layer with 32 filters of size 3x3\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay),\n",
        "                 input_shape=X_train.shape[1:]))\n",
        "# Add batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the second convolutional layer similar to the first\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "# Add the first max pooling layer with pool size of 2x2\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Add dropout layer with 0.2 dropout rate\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "# Add the third and fourth convolutional layers with 64 filters\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the second max pooling layer and increase dropout rate to 0.3\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(rate=0.3))\n",
        "# Add the fifth and sixth convolutional layers with 128 filters\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the third max pooling layer and increase dropout rate to 0.4\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(rate=0.4))\n",
        "\n",
        "# Add the seventh and eighth convolutional layers with 256 filters\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "# Add the fourth max pooling layer and increase dropout rate to 0.5\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(rate=0.5))\n",
        "\n",
        "# Flatten the tensor output from the previous layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer with softmax activation function for outputting class probabilities\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "w_Q7r-enU7JY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl4VdtVBVMeM",
        "outputId": "22f76226-e972-4301-81d1-825b5eb80edc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 32, 32, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 4, 4, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 4, 4, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1186346 (4.53 MB)\n",
            "Trainable params: 1184426 (4.52 MB)\n",
            "Non-trainable params: 1920 (7.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 | Training the CNN Model**"
      ],
      "metadata": {
        "id": "PxRQKFiumbT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training uses a batch size of 64 and will run for a maximum of 100 epochs or until the early stopping condition is met. During the training, the model's performance is evaluated on the validation data after each epoch."
      ],
      "metadata": {
        "id": "10YdMpMomg0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The ReduceLROnPlateau** callback is used to reduce the learning rate by half (factor=0.5) whenever the validation loss does not improve for 10 consecutive epochs. This helps to adjust the learning rate dynamically, allowing the model to get closer to the global minimum of the loss function when progress has plateaued. This strategy can improve the convergence of the training process.\n",
        "\n",
        "\n",
        "\n",
        "**The EarlyStopping** callback is employed to monitor the validation loss and halt the training process when there hasn't been any improvement for a certain number of epochs, ensuring that the model doesn't waste computational resources and time. Furthermore, this callback restores the best weights from the training process, ensuring we conclude with the optimal model configuration from the epochs."
      ],
      "metadata": {
        "id": "urfsCPUMmr1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Set the batch size for the training\n",
        "batch_size = 64\n",
        "\n",
        "# Set the maximum number of epochs for the training\n",
        "epochs = 100\n",
        "\n",
        "# Define the optimizer (Adam)\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "# Compile the model with the defined optimizer, loss function, and metrics\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Add ReduceLROnPlateau callback\n",
        "# Here, the learning rate will be reduced by half (factor=0.5) if no improvement in validation loss is observed for 10 epochs\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001)\n",
        "\n",
        "# Add EarlyStopping callback\n",
        "# Here, training will be stopped if no improvement in validation loss is observed for 40 epochs.\n",
        "# The `restore_best_weights` parameter ensures that the model weights are reset to the values from the epoch\n",
        "# with the best value of the monitored quantity (in this case, 'val_loss').\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)\n",
        "# Fit the model on the training data, using the defined batch size and number of epochs\n",
        "# The validation data is used to evaluate the model's performance during training\n",
        "# The callbacks implemented are learning rate reduction when a plateau is reached in validation loss and\n",
        "# stopping training early if no improvement is observed\n",
        "model.fit(data_generator.flow(X_train, y_train, batch_size=batch_size),\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=[reduce_lr, early_stopping],\n",
        "          verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0GYqMeBVR6b",
        "outputId": "8bb3c31b-060c-40ee-f6a6-75b36b2cb7f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "704/704 - 70s - loss: 2.3118 - accuracy: 0.3459 - val_loss: 1.7809 - val_accuracy: 0.4456 - lr: 5.0000e-04 - 70s/epoch - 99ms/step\n",
            "Epoch 2/100\n",
            "704/704 - 56s - loss: 1.7191 - accuracy: 0.4733 - val_loss: 1.9492 - val_accuracy: 0.5094 - lr: 5.0000e-04 - 56s/epoch - 79ms/step\n",
            "Epoch 3/100\n",
            "704/704 - 57s - loss: 1.4993 - accuracy: 0.5386 - val_loss: 1.2994 - val_accuracy: 0.5892 - lr: 5.0000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 4/100\n",
            "704/704 - 56s - loss: 1.3198 - accuracy: 0.5855 - val_loss: 1.2750 - val_accuracy: 0.6198 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 5/100\n",
            "704/704 - 56s - loss: 1.2123 - accuracy: 0.6169 - val_loss: 1.0366 - val_accuracy: 0.6830 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 6/100\n",
            "704/704 - 57s - loss: 1.1293 - accuracy: 0.6476 - val_loss: 1.0838 - val_accuracy: 0.6784 - lr: 5.0000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 7/100\n",
            "704/704 - 57s - loss: 1.0581 - accuracy: 0.6728 - val_loss: 0.9762 - val_accuracy: 0.7048 - lr: 5.0000e-04 - 57s/epoch - 80ms/step\n",
            "Epoch 8/100\n",
            "704/704 - 55s - loss: 1.0074 - accuracy: 0.6913 - val_loss: 0.8844 - val_accuracy: 0.7416 - lr: 5.0000e-04 - 55s/epoch - 78ms/step\n",
            "Epoch 9/100\n",
            "704/704 - 57s - loss: 0.9757 - accuracy: 0.7057 - val_loss: 0.9486 - val_accuracy: 0.7224 - lr: 5.0000e-04 - 57s/epoch - 80ms/step\n",
            "Epoch 10/100\n",
            "704/704 - 55s - loss: 0.9232 - accuracy: 0.7266 - val_loss: 0.7820 - val_accuracy: 0.7752 - lr: 5.0000e-04 - 55s/epoch - 78ms/step\n",
            "Epoch 11/100\n",
            "704/704 - 55s - loss: 0.9053 - accuracy: 0.7349 - val_loss: 0.8522 - val_accuracy: 0.7562 - lr: 5.0000e-04 - 55s/epoch - 78ms/step\n",
            "Epoch 12/100\n",
            "704/704 - 56s - loss: 0.8896 - accuracy: 0.7447 - val_loss: 0.8062 - val_accuracy: 0.7778 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 13/100\n",
            "704/704 - 55s - loss: 0.8696 - accuracy: 0.7512 - val_loss: 0.7849 - val_accuracy: 0.7802 - lr: 5.0000e-04 - 55s/epoch - 78ms/step\n",
            "Epoch 14/100\n",
            "704/704 - 55s - loss: 0.8531 - accuracy: 0.7605 - val_loss: 0.8160 - val_accuracy: 0.7828 - lr: 5.0000e-04 - 55s/epoch - 78ms/step\n",
            "Epoch 15/100\n",
            "704/704 - 57s - loss: 0.8491 - accuracy: 0.7628 - val_loss: 0.7861 - val_accuracy: 0.7898 - lr: 5.0000e-04 - 57s/epoch - 80ms/step\n",
            "Epoch 16/100\n",
            "704/704 - 55s - loss: 0.8369 - accuracy: 0.7684 - val_loss: 0.7560 - val_accuracy: 0.7964 - lr: 5.0000e-04 - 55s/epoch - 78ms/step\n",
            "Epoch 17/100\n",
            "704/704 - 56s - loss: 0.8278 - accuracy: 0.7740 - val_loss: 0.7262 - val_accuracy: 0.8076 - lr: 5.0000e-04 - 56s/epoch - 79ms/step\n",
            "Epoch 18/100\n",
            "704/704 - 59s - loss: 0.8108 - accuracy: 0.7798 - val_loss: 0.7175 - val_accuracy: 0.8162 - lr: 5.0000e-04 - 59s/epoch - 83ms/step\n",
            "Epoch 19/100\n",
            "704/704 - 56s - loss: 0.8106 - accuracy: 0.7828 - val_loss: 0.7169 - val_accuracy: 0.8162 - lr: 5.0000e-04 - 56s/epoch - 79ms/step\n",
            "Epoch 20/100\n",
            "704/704 - 58s - loss: 0.7889 - accuracy: 0.7890 - val_loss: 0.7329 - val_accuracy: 0.8124 - lr: 5.0000e-04 - 58s/epoch - 83ms/step\n",
            "Epoch 21/100\n",
            "704/704 - 57s - loss: 0.7837 - accuracy: 0.7920 - val_loss: 0.7392 - val_accuracy: 0.8084 - lr: 5.0000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 22/100\n",
            "704/704 - 57s - loss: 0.8083 - accuracy: 0.7868 - val_loss: 0.7277 - val_accuracy: 0.8136 - lr: 5.0000e-04 - 57s/epoch - 80ms/step\n",
            "Epoch 23/100\n",
            "704/704 - 57s - loss: 0.7937 - accuracy: 0.7930 - val_loss: 0.7040 - val_accuracy: 0.8264 - lr: 5.0000e-04 - 57s/epoch - 80ms/step\n",
            "Epoch 24/100\n",
            "704/704 - 58s - loss: 0.7684 - accuracy: 0.7999 - val_loss: 0.7105 - val_accuracy: 0.8242 - lr: 5.0000e-04 - 58s/epoch - 82ms/step\n",
            "Epoch 25/100\n",
            "704/704 - 57s - loss: 0.7628 - accuracy: 0.8013 - val_loss: 0.7354 - val_accuracy: 0.8182 - lr: 5.0000e-04 - 57s/epoch - 80ms/step\n",
            "Epoch 26/100\n",
            "704/704 - 57s - loss: 0.7548 - accuracy: 0.8071 - val_loss: 0.7094 - val_accuracy: 0.8240 - lr: 5.0000e-04 - 57s/epoch - 82ms/step\n",
            "Epoch 27/100\n",
            "704/704 - 56s - loss: 0.7590 - accuracy: 0.8062 - val_loss: 0.7185 - val_accuracy: 0.8198 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 28/100\n",
            "704/704 - 56s - loss: 0.7591 - accuracy: 0.8074 - val_loss: 0.7136 - val_accuracy: 0.8256 - lr: 5.0000e-04 - 56s/epoch - 79ms/step\n",
            "Epoch 29/100\n",
            "704/704 - 58s - loss: 0.7506 - accuracy: 0.8116 - val_loss: 0.6891 - val_accuracy: 0.8344 - lr: 5.0000e-04 - 58s/epoch - 83ms/step\n",
            "Epoch 30/100\n",
            "704/704 - 56s - loss: 0.7371 - accuracy: 0.8147 - val_loss: 0.7495 - val_accuracy: 0.8164 - lr: 5.0000e-04 - 56s/epoch - 79ms/step\n",
            "Epoch 31/100\n",
            "704/704 - 56s - loss: 0.7411 - accuracy: 0.8158 - val_loss: 0.6679 - val_accuracy: 0.8370 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 32/100\n",
            "704/704 - 57s - loss: 0.7491 - accuracy: 0.8139 - val_loss: 0.7250 - val_accuracy: 0.8282 - lr: 5.0000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 33/100\n",
            "704/704 - 56s - loss: 0.7348 - accuracy: 0.8188 - val_loss: 0.7154 - val_accuracy: 0.8320 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 34/100\n",
            "704/704 - 58s - loss: 0.7260 - accuracy: 0.8212 - val_loss: 0.6858 - val_accuracy: 0.8374 - lr: 5.0000e-04 - 58s/epoch - 82ms/step\n",
            "Epoch 35/100\n",
            "704/704 - 57s - loss: 0.7336 - accuracy: 0.8186 - val_loss: 0.6640 - val_accuracy: 0.8440 - lr: 5.0000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 36/100\n",
            "704/704 - 56s - loss: 0.7317 - accuracy: 0.8203 - val_loss: 0.7087 - val_accuracy: 0.8370 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 37/100\n",
            "704/704 - 59s - loss: 0.7237 - accuracy: 0.8238 - val_loss: 0.6710 - val_accuracy: 0.8478 - lr: 5.0000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 38/100\n",
            "704/704 - 56s - loss: 0.7316 - accuracy: 0.8205 - val_loss: 0.7043 - val_accuracy: 0.8324 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 39/100\n",
            "704/704 - 57s - loss: 0.7129 - accuracy: 0.8288 - val_loss: 0.7183 - val_accuracy: 0.8344 - lr: 5.0000e-04 - 57s/epoch - 80ms/step\n",
            "Epoch 40/100\n",
            "704/704 - 59s - loss: 0.7207 - accuracy: 0.8241 - val_loss: 0.7192 - val_accuracy: 0.8286 - lr: 5.0000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 41/100\n",
            "704/704 - 57s - loss: 0.7110 - accuracy: 0.8282 - val_loss: 0.6634 - val_accuracy: 0.8518 - lr: 5.0000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 42/100\n",
            "704/704 - 58s - loss: 0.7035 - accuracy: 0.8318 - val_loss: 0.7115 - val_accuracy: 0.8292 - lr: 5.0000e-04 - 58s/epoch - 82ms/step\n",
            "Epoch 43/100\n",
            "704/704 - 56s - loss: 0.7075 - accuracy: 0.8295 - val_loss: 0.7046 - val_accuracy: 0.8390 - lr: 5.0000e-04 - 56s/epoch - 79ms/step\n",
            "Epoch 44/100\n",
            "704/704 - 56s - loss: 0.7027 - accuracy: 0.8348 - val_loss: 0.6449 - val_accuracy: 0.8520 - lr: 5.0000e-04 - 56s/epoch - 79ms/step\n",
            "Epoch 45/100\n",
            "704/704 - 58s - loss: 0.6951 - accuracy: 0.8350 - val_loss: 0.6358 - val_accuracy: 0.8570 - lr: 5.0000e-04 - 58s/epoch - 83ms/step\n",
            "Epoch 46/100\n",
            "704/704 - 56s - loss: 0.6839 - accuracy: 0.8372 - val_loss: 0.6519 - val_accuracy: 0.8534 - lr: 5.0000e-04 - 56s/epoch - 80ms/step\n",
            "Epoch 47/100\n",
            "704/704 - 59s - loss: 0.6934 - accuracy: 0.8371 - val_loss: 0.7075 - val_accuracy: 0.8364 - lr: 5.0000e-04 - 59s/epoch - 85ms/step\n",
            "Epoch 48/100\n",
            "704/704 - 60s - loss: 0.6856 - accuracy: 0.8398 - val_loss: 0.6630 - val_accuracy: 0.8494 - lr: 5.0000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 49/100\n",
            "704/704 - 66s - loss: 0.6867 - accuracy: 0.8400 - val_loss: 0.6197 - val_accuracy: 0.8608 - lr: 5.0000e-04 - 66s/epoch - 94ms/step\n",
            "Epoch 50/100\n",
            "704/704 - 64s - loss: 0.7015 - accuracy: 0.8341 - val_loss: 0.6964 - val_accuracy: 0.8408 - lr: 5.0000e-04 - 64s/epoch - 91ms/step\n",
            "Epoch 51/100\n",
            "704/704 - 68s - loss: 0.6807 - accuracy: 0.8412 - val_loss: 0.6253 - val_accuracy: 0.8582 - lr: 5.0000e-04 - 68s/epoch - 97ms/step\n",
            "Epoch 52/100\n",
            "704/704 - 73s - loss: 0.6808 - accuracy: 0.8421 - val_loss: 0.6535 - val_accuracy: 0.8556 - lr: 5.0000e-04 - 73s/epoch - 103ms/step\n",
            "Epoch 53/100\n",
            "704/704 - 63s - loss: 0.6829 - accuracy: 0.8428 - val_loss: 0.6139 - val_accuracy: 0.8624 - lr: 5.0000e-04 - 63s/epoch - 89ms/step\n",
            "Epoch 54/100\n",
            "704/704 - 68s - loss: 0.6755 - accuracy: 0.8426 - val_loss: 0.6319 - val_accuracy: 0.8588 - lr: 5.0000e-04 - 68s/epoch - 96ms/step\n",
            "Epoch 55/100\n",
            "704/704 - 62s - loss: 0.6711 - accuracy: 0.8432 - val_loss: 0.6683 - val_accuracy: 0.8494 - lr: 5.0000e-04 - 62s/epoch - 88ms/step\n",
            "Epoch 56/100\n",
            "704/704 - 68s - loss: 0.6791 - accuracy: 0.8409 - val_loss: 0.6460 - val_accuracy: 0.8502 - lr: 5.0000e-04 - 68s/epoch - 97ms/step\n",
            "Epoch 57/100\n",
            "704/704 - 64s - loss: 0.6681 - accuracy: 0.8451 - val_loss: 0.6185 - val_accuracy: 0.8604 - lr: 5.0000e-04 - 64s/epoch - 91ms/step\n",
            "Epoch 58/100\n",
            "704/704 - 69s - loss: 0.6621 - accuracy: 0.8466 - val_loss: 0.6207 - val_accuracy: 0.8642 - lr: 5.0000e-04 - 69s/epoch - 98ms/step\n",
            "Epoch 59/100\n",
            "704/704 - 61s - loss: 0.6721 - accuracy: 0.8440 - val_loss: 0.6884 - val_accuracy: 0.8454 - lr: 5.0000e-04 - 61s/epoch - 86ms/step\n",
            "Epoch 60/100\n",
            "704/704 - 65s - loss: 0.6818 - accuracy: 0.8412 - val_loss: 0.5863 - val_accuracy: 0.8754 - lr: 5.0000e-04 - 65s/epoch - 92ms/step\n",
            "Epoch 61/100\n",
            "704/704 - 65s - loss: 0.6648 - accuracy: 0.8476 - val_loss: 0.6034 - val_accuracy: 0.8706 - lr: 5.0000e-04 - 65s/epoch - 93ms/step\n",
            "Epoch 62/100\n",
            "704/704 - 66s - loss: 0.6577 - accuracy: 0.8504 - val_loss: 0.6474 - val_accuracy: 0.8550 - lr: 5.0000e-04 - 66s/epoch - 93ms/step\n",
            "Epoch 63/100\n",
            "704/704 - 62s - loss: 0.6572 - accuracy: 0.8477 - val_loss: 0.5976 - val_accuracy: 0.8676 - lr: 5.0000e-04 - 62s/epoch - 88ms/step\n",
            "Epoch 64/100\n",
            "704/704 - 60s - loss: 0.6596 - accuracy: 0.8502 - val_loss: 0.5955 - val_accuracy: 0.8670 - lr: 5.0000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 65/100\n",
            "704/704 - 61s - loss: 0.6538 - accuracy: 0.8504 - val_loss: 0.5998 - val_accuracy: 0.8742 - lr: 5.0000e-04 - 61s/epoch - 86ms/step\n",
            "Epoch 66/100\n",
            "704/704 - 59s - loss: 0.6547 - accuracy: 0.8487 - val_loss: 0.6070 - val_accuracy: 0.8716 - lr: 5.0000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 67/100\n",
            "704/704 - 60s - loss: 0.6547 - accuracy: 0.8519 - val_loss: 0.6531 - val_accuracy: 0.8552 - lr: 5.0000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 68/100\n",
            "704/704 - 60s - loss: 0.6611 - accuracy: 0.8474 - val_loss: 0.6575 - val_accuracy: 0.8576 - lr: 5.0000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 69/100\n",
            "704/704 - 59s - loss: 0.6435 - accuracy: 0.8555 - val_loss: 0.5991 - val_accuracy: 0.8702 - lr: 5.0000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 70/100\n",
            "704/704 - 60s - loss: 0.6437 - accuracy: 0.8540 - val_loss: 0.5940 - val_accuracy: 0.8758 - lr: 5.0000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 71/100\n",
            "704/704 - 60s - loss: 0.6084 - accuracy: 0.8658 - val_loss: 0.5739 - val_accuracy: 0.8800 - lr: 2.5000e-04 - 60s/epoch - 86ms/step\n",
            "Epoch 72/100\n",
            "704/704 - 59s - loss: 0.5908 - accuracy: 0.8687 - val_loss: 0.5785 - val_accuracy: 0.8760 - lr: 2.5000e-04 - 59s/epoch - 83ms/step\n",
            "Epoch 73/100\n",
            "704/704 - 60s - loss: 0.5760 - accuracy: 0.8731 - val_loss: 0.5729 - val_accuracy: 0.8788 - lr: 2.5000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 74/100\n",
            "704/704 - 60s - loss: 0.5718 - accuracy: 0.8728 - val_loss: 0.5780 - val_accuracy: 0.8748 - lr: 2.5000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 75/100\n",
            "704/704 - 62s - loss: 0.5616 - accuracy: 0.8770 - val_loss: 0.5623 - val_accuracy: 0.8784 - lr: 2.5000e-04 - 62s/epoch - 87ms/step\n",
            "Epoch 76/100\n",
            "704/704 - 60s - loss: 0.5542 - accuracy: 0.8793 - val_loss: 0.5719 - val_accuracy: 0.8728 - lr: 2.5000e-04 - 60s/epoch - 86ms/step\n",
            "Epoch 77/100\n",
            "704/704 - 60s - loss: 0.5559 - accuracy: 0.8745 - val_loss: 0.5534 - val_accuracy: 0.8826 - lr: 2.5000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 78/100\n",
            "704/704 - 61s - loss: 0.5525 - accuracy: 0.8752 - val_loss: 0.5224 - val_accuracy: 0.8892 - lr: 2.5000e-04 - 61s/epoch - 87ms/step\n",
            "Epoch 79/100\n",
            "704/704 - 61s - loss: 0.5376 - accuracy: 0.8784 - val_loss: 0.5519 - val_accuracy: 0.8788 - lr: 2.5000e-04 - 61s/epoch - 87ms/step\n",
            "Epoch 80/100\n",
            "704/704 - 61s - loss: 0.5399 - accuracy: 0.8768 - val_loss: 0.5404 - val_accuracy: 0.8792 - lr: 2.5000e-04 - 61s/epoch - 86ms/step\n",
            "Epoch 81/100\n",
            "704/704 - 59s - loss: 0.5427 - accuracy: 0.8759 - val_loss: 0.5366 - val_accuracy: 0.8780 - lr: 2.5000e-04 - 59s/epoch - 83ms/step\n",
            "Epoch 82/100\n",
            "704/704 - 59s - loss: 0.5298 - accuracy: 0.8819 - val_loss: 0.5718 - val_accuracy: 0.8732 - lr: 2.5000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 83/100\n",
            "704/704 - 58s - loss: 0.5344 - accuracy: 0.8782 - val_loss: 0.5179 - val_accuracy: 0.8912 - lr: 2.5000e-04 - 58s/epoch - 82ms/step\n",
            "Epoch 84/100\n",
            "704/704 - 60s - loss: 0.5298 - accuracy: 0.8789 - val_loss: 0.5203 - val_accuracy: 0.8888 - lr: 2.5000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 85/100\n",
            "704/704 - 60s - loss: 0.5233 - accuracy: 0.8796 - val_loss: 0.5203 - val_accuracy: 0.8874 - lr: 2.5000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 86/100\n",
            "704/704 - 59s - loss: 0.5184 - accuracy: 0.8836 - val_loss: 0.5130 - val_accuracy: 0.8908 - lr: 2.5000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 87/100\n",
            "704/704 - 58s - loss: 0.5188 - accuracy: 0.8816 - val_loss: 0.5351 - val_accuracy: 0.8806 - lr: 2.5000e-04 - 58s/epoch - 83ms/step\n",
            "Epoch 88/100\n",
            "704/704 - 58s - loss: 0.5161 - accuracy: 0.8821 - val_loss: 0.5359 - val_accuracy: 0.8836 - lr: 2.5000e-04 - 58s/epoch - 82ms/step\n",
            "Epoch 89/100\n",
            "704/704 - 60s - loss: 0.5169 - accuracy: 0.8819 - val_loss: 0.5049 - val_accuracy: 0.8872 - lr: 2.5000e-04 - 60s/epoch - 85ms/step\n",
            "Epoch 90/100\n",
            "704/704 - 58s - loss: 0.5138 - accuracy: 0.8820 - val_loss: 0.5371 - val_accuracy: 0.8808 - lr: 2.5000e-04 - 58s/epoch - 83ms/step\n",
            "Epoch 91/100\n",
            "704/704 - 58s - loss: 0.5093 - accuracy: 0.8838 - val_loss: 0.5241 - val_accuracy: 0.8864 - lr: 2.5000e-04 - 58s/epoch - 82ms/step\n",
            "Epoch 92/100\n",
            "704/704 - 61s - loss: 0.5029 - accuracy: 0.8852 - val_loss: 0.4967 - val_accuracy: 0.8918 - lr: 2.5000e-04 - 61s/epoch - 86ms/step\n",
            "Epoch 93/100\n",
            "704/704 - 58s - loss: 0.5136 - accuracy: 0.8812 - val_loss: 0.4948 - val_accuracy: 0.8896 - lr: 2.5000e-04 - 58s/epoch - 82ms/step\n",
            "Epoch 94/100\n",
            "704/704 - 57s - loss: 0.5042 - accuracy: 0.8840 - val_loss: 0.4966 - val_accuracy: 0.8898 - lr: 2.5000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 95/100\n",
            "704/704 - 59s - loss: 0.5013 - accuracy: 0.8862 - val_loss: 0.5351 - val_accuracy: 0.8804 - lr: 2.5000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 96/100\n",
            "704/704 - 59s - loss: 0.5020 - accuracy: 0.8842 - val_loss: 0.5100 - val_accuracy: 0.8890 - lr: 2.5000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 97/100\n",
            "704/704 - 59s - loss: 0.5037 - accuracy: 0.8851 - val_loss: 0.5269 - val_accuracy: 0.8762 - lr: 2.5000e-04 - 59s/epoch - 84ms/step\n",
            "Epoch 98/100\n",
            "704/704 - 58s - loss: 0.5015 - accuracy: 0.8833 - val_loss: 0.4863 - val_accuracy: 0.8924 - lr: 2.5000e-04 - 58s/epoch - 83ms/step\n",
            "Epoch 99/100\n",
            "704/704 - 57s - loss: 0.4968 - accuracy: 0.8865 - val_loss: 0.4855 - val_accuracy: 0.8908 - lr: 2.5000e-04 - 57s/epoch - 81ms/step\n",
            "Epoch 100/100\n",
            "704/704 - 56s - loss: 0.4957 - accuracy: 0.8864 - val_loss: 0.4959 - val_accuracy: 0.8910 - lr: 2.5000e-04 - 56s/epoch - 79ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d18de626bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6 | Visualizing the Learning Curves**"
      ],
      "metadata": {
        "id": "L-70Z4DKm_D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(model.history.history['loss'], label='Train Loss', color='#8502d1')\n",
        "plt.plot(model.history.history['val_loss'], label='Validation Loss', color='darkorange')\n",
        "plt.legend()\n",
        "plt.title('Loss Evolution')\n",
        "\n",
        "# Plotting the training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(model.history.history['accuracy'], label='Train Accuracy', color='#8502d1')\n",
        "plt.plot(model.history.history['val_accuracy'], label='Validation Accuracy', color='darkorange')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Evolution')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OR1dxtPT0Mn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7 | Evaluating the Optimal Model on Test Data**"
      ],
      "metadata": {
        "id": "KJtrSNvwnRAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print('\\nTest Accuracy:', test_acc)\n",
        "print('Test Loss:    ', test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFRd7WcinSk0",
        "outputId": "34d8a4da-3074-44e0-9734-f70876976fbd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 0.5042 - accuracy: 0.8914\n",
            "\n",
            "Test Accuracy: 0.8913999795913696\n",
            "Test Loss:     0.5042402148246765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print('\\nTest Accuracy:', test_acc)\n",
        "print('Test Loss:    ', test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAktvVwhy5wS",
        "outputId": "1b9347d0-a0db-4e78-e5c8-6eb2a7359c61"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.5042 - accuracy: 0.8914\n",
            "\n",
            "Test Accuracy: 0.8913999795913696\n",
            "Test Loss:     0.5042402148246765\n"
          ]
        }
      ]
    }
  ]
}